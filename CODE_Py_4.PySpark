################ PySpark is the Python API for Spark

# Good Resource:
# http://spark.apache.org/docs/2.1.0/api/python/pyspark.html

# Splitting data and compute power across many clusters and nodes speeds up everything
# 'Master' controls everything : slaves
# Spark's core data structure is an RDD -- Resilient Distributed Dataset
# Work with Spark DataFrames rather than RDDs! It's just easier...

#####################################################
#################### SET UP CONNECTION

# 1: Set up Spark Connection to cluster
sc = SparkContext(conf=conf)

# 2: Set up Spark Session to interface into the connection
from pyspark.sql import SparkSession
my_spark = SparkSession.builder.getOrCreate()


#####################################################
#################### LOOKING AT DATA

# Print the tables in the catalog
print(spark.catalog.listTables())

### RUN A QUERY
# Don't change this query
query = "FROM flights SELECT * LIMIT 10"
# Get the first 10 rows of flights
flights10 = spark.sql(query)
# Show the results
flights10.show()

### CHANGE TO PANDAS DF FROM SPARK CLUSTER
# Don't change this query
query = "SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest"
# Run the query
flight_counts = spark.sql(query)
# Convert the results to a pandas DataFrame
pd_counts = flight_counts.toPandas()
# Print the head of pd_counts
print(pd_counts.head())
